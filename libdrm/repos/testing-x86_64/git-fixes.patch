From efa485b2cf2f1f46760a3d9297ba1a73a2dbcd05 Mon Sep 17 00:00:00 2001
From: Eric Anholt <eric@anholt.net>
Date: Wed, 25 Feb 2009 05:36:56 +0000
Subject: intel: Don't copy dirty data out when freeing a BO in the fake bufmgr.

---
diff --git a/libdrm/intel/intel_bufmgr_fake.c b/libdrm/intel/intel_bufmgr_fake.c
index e7cec35..e1f98d6 100644
--- a/libdrm/intel/intel_bufmgr_fake.c
+++ b/libdrm/intel/intel_bufmgr_fake.c
@@ -444,7 +444,8 @@ alloc_block(drm_intel_bo *bo)
 
 /* Release the card storage associated with buf:
  */
-static void free_block(drm_intel_bufmgr_fake *bufmgr_fake, struct block *block)
+static void free_block(drm_intel_bufmgr_fake *bufmgr_fake, struct block *block,
+		       int skip_dirty_copy)
 {
    drm_intel_bo_fake *bo_fake;
    DBG("free block %p %08x %d %d\n", block, block->mem->ofs, block->on_hardware, block->fenced);
@@ -453,7 +454,11 @@ static void free_block(drm_intel_bufmgr_fake *bufmgr_fake, struct block *block)
       return;
 
    bo_fake = (drm_intel_bo_fake *)block->bo;
-   if (!(bo_fake->flags & (BM_PINNED | BM_NO_BACKING_STORE)) && (bo_fake->card_dirty == 1)) {
+
+   if (bo_fake->flags & (BM_PINNED | BM_NO_BACKING_STORE))
+      skip_dirty_copy = 1;
+
+   if (!skip_dirty_copy && (bo_fake->card_dirty == 1)) {
      memcpy(bo_fake->backing_store, block->virtual, block->bo->size);
      bo_fake->card_dirty = 0;
      bo_fake->dirty = 1;
@@ -534,7 +539,7 @@ evict_lru(drm_intel_bufmgr_fake *bufmgr_fake, unsigned int max_fence)
       set_dirty(&bo_fake->bo);
       bo_fake->block = NULL;
 
-      free_block(bufmgr_fake, block);
+      free_block(bufmgr_fake, block, 0);
       return 1;
    }
 
@@ -557,7 +562,7 @@ evict_mru(drm_intel_bufmgr_fake *bufmgr_fake)
       set_dirty(&bo_fake->bo);
       bo_fake->block = NULL;
 
-      free_block(bufmgr_fake, block);
+      free_block(bufmgr_fake, block, 0);
       return 1;
    }
 
@@ -872,7 +877,7 @@ drm_intel_fake_bo_unreference_locked(drm_intel_bo *bo)
       assert(bo_fake->map_count == 0);
       /* No remaining references, so free it */
       if (bo_fake->block)
-	 free_block(bufmgr_fake, bo_fake->block);
+	 free_block(bufmgr_fake, bo_fake->block, 1);
       free_backing_store(bo);
 
       for (i = 0; i < bo_fake->nr_relocs; i++)
@@ -1064,7 +1069,7 @@ drm_intel_fake_kick_all_locked(drm_intel_bufmgr_fake *bufmgr_fake)
       drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *)block->bo;
 
       block->on_hardware = 0;
-      free_block(bufmgr_fake, block);
+      free_block(bufmgr_fake, block, 0);
       bo_fake->block = NULL;
       bo_fake->validated = 0;
       if (!(bo_fake->flags & BM_NO_BACKING_STORE))
@@ -1463,7 +1468,7 @@ drm_intel_bufmgr_fake_evict_all(drm_intel_bufmgr *bufmgr)
 
    DRMLISTFOREACHSAFE(block, tmp, &bufmgr_fake->lru) {
       /* Releases the memory, and memcpys dirty contents out if necessary. */
-      free_block(bufmgr_fake, block);
+      free_block(bufmgr_fake, block, 0);
    }
 
    pthread_mutex_unlock(&bufmgr_fake->lock);
--
cgit v0.8.2
From 7ce8d4c1fec618ac2578ea0ddb8915b1b41ab9cb Mon Sep 17 00:00:00 2001
From: Eric Anholt <eric@anholt.net>
Date: Fri, 27 Feb 2009 21:46:31 +0000
Subject: intel: Update reloc_tree_size of the first buffer when we count the tree size.

This helps avoid the n^2 performance cost of counting tree size when we
get a lot of relocations into our batch buffer.  rgb10text on keithp's laptop
went from 136k glyphs/sec to 234k glyphs/sec.
---
diff --git a/libdrm/intel/intel_bufmgr_gem.c b/libdrm/intel/intel_bufmgr_gem.c
index 6ddecf4..9e49d7c 100644
--- a/libdrm/intel/intel_bufmgr_gem.c
+++ b/libdrm/intel/intel_bufmgr_gem.c
@@ -1260,8 +1260,21 @@ drm_intel_gem_compute_batch_space(drm_intel_bo **bo_array, int count)
     int i;
     unsigned int total = 0;
 
-    for (i = 0; i < count; i++)
+    for (i = 0; i < count; i++) {
 	total += drm_intel_gem_bo_get_aperture_space(bo_array[i]);
+	/* For the first buffer object in the array, we get an accurate count
+	 * back for its reloc_tree size (since nothing had been flagged as
+	 * being counted yet).  We can save that value out as a more
+	 * conservative reloc_tree_size that avoids double-counting target
+	 * buffers.  Since the first buffer happens to usually be the batch
+	 * buffer in our callers, this can pull us back from doing the tree
+	 * walk on every new batch emit.
+	 */
+	if (i == 0) {
+	    drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo_array[i];
+	    bo_gem->reloc_tree_size = total;
+	}
+    }
 
     for (i = 0; i < count; i++)
 	drm_intel_gem_bo_clear_aperture_space_flag(bo_array[i]);
--
cgit v0.8.2
